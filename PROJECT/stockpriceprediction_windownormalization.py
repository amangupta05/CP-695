# -*- coding: utf-8 -*-
"""StockPricePrediction_WindowNormalization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XydKbRUmJaYz734HB1YJp5VyHKJ7VW9c
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense

# Load the data
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RELIANCE.NS.csv', date_parser=True)
df = df.sort_values('Date')

df.info()

df.head()

from matplotlib import pyplot as plt

plt.plot_date(df['Date'],df['Close'])

plt.hist(df['Close'])

plt.scatter(df['Date'],df['Close'])

import plotly.express as px

fig = px.violin(df, y="Close")
fig.show()

train_data = np.array(df[['Close',"Open","High","Low"]])
train_data.shape

train_data = train_data.reshape(-1,1)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
chunk_size = 250

for i in range(0, train_data.shape[0], chunk_size):
    train_data[i:i+chunk_size]=scaler.fit_transform(train_data[i:i+chunk_size])

import seaborn as sns

sns.violinplot(train_data)
data = df.sort_values('Date')

# Define the number of time steps
time_steps = 60

# Split the data into training and test sets
train_data = data[:int(len(data) * 0.8)]
test_data = data[int(len(data) * 0.8) - time_steps:]

# Normalize the data
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_data[['Close']])
test_scaled = scaler.transform(test_data[['Close']])

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

def scale_data_in_chunks(train_data,chunk_size):
  for i in range(0, train_data.shape[0], chunk_size):
    train_data[i:i+chunk_size]=scaler.fit_transform(train_data[i:i+chunk_size])
  return train_data

df = df.sort_values('Date')

# Define the number of time steps
time_steps = 60

# Split the data into training and test sets
train_data = df[:int(len(data) * 0.8)]
test_data = df[int(len(data) * 0.8) - time_steps:]

train_data.shape,test_data.shape# Visualize the results
#sns.violinplot(test_data.shape)
#sns.violinplot(train_data.shape)

train_scaled = scale_data_in_chunks(np.array(train_data[['Close',"Open","High","Low"]]).reshape(-1,1),200)
#test_scaled = scale_data_in_chunks(test_data,)
# Normalize the data
scaler = MinMaxScaler()
#train_scaled = scaler.fit_transform(train_data[['Close']])
test_scaled = scaler.fit_transform(np.array(test_data[['Close',"Open","High","Low"]]).reshape(-1,1),200)

sns.violinplot(train_scaled)

sns.violinplot(test_scaled)

# Prepare the training data
X_train = []
y_train = []
for i in range(time_steps, len(train_scaled)):
    X_train.append(train_scaled[i - time_steps:i, 0])
    y_train.append(train_scaled[i, 0])
X_train, y_train = np.array(X_train), np.array(y_train)
# X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# Prepare the test data
X_test = []
y_test = []
for i in range(time_steps, len(test_scaled)):
    X_test.append(test_scaled[i - time_steps:i, 0])
    y_test.append(test_scaled[i, 0])
X_test, y_test = np.array(X_test), np.array(y_test)
# X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

X_train.shape,y_train.shape

X_test.shape

"""##LSTM"""

# Create the LSTM model
model = Sequential()
model.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=64, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=64))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=32)

# Make predictions
y_pred = scaler.inverse_transform(model.predict(X_test))
y_test.reshape(-1,1)
# Evaluate the model
score = model.evaluate(y_pred, y_test)

y_pred.shape

y_test.reshape(2156,1)

print(score)

# Create a new DataFrame to store the predictions
predictions = test_data.iloc[time_steps:].copy()
predictions['Predicted'] = y_pred.reshape(-1)

# Visualize the actual and predicted prices
plt.plot(predictions['Date'], predictions['Close'], color='blue', label='Actual Price')
plt.plot(predictions['Date'], predictions['Predicted'], color='red', label='Predicted Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Actual vs. Predicted Prices')
plt.legend()
plt.show()

# Make predictions for the last window in the test data
last_window = X_test[-1]
last_window = last_window.reshape(1, last_window.shape[0], 1)


y_pred = scaler.inverse_transform(model.predict(last_window)).reshape(-1)

# Create a new DataFrame to store the predictions
predictions = pd.DataFrame({
    'Date': test_data.index[-1] + pd.DateOffset(1),
    'Actual': test_data['Close'][-1],
    'Predicted': y_pred
})

# Visualize the actual and predicted prices
sns.lineplot(data=test_data[-100:], x='Date', y='Close')
sns.lineplot(data=predictions, x='Date', y='Predicted')

# Create the LSTM model
model = Sequential()
model.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=64, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=64))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=32)

# Make predictions
y_pred = scaler.inverse_transform(model.predict(X_test))

# Evaluate the model
score = model.evaluate(X_test, y_test)

# Visualize the results
import matplotlib.pyplot as plt
plt.plot(test_data['Date'].iloc[time_steps:], test_data['Close'].iloc[time_steps:], color='blue', label='Actual Price')
plt.plot(test_data['Date'].iloc[time_steps:], y_pred, color='red', label='Predicted Price')
plt.xticks(np.arange(0, len(test_data)-time_steps, step=50), test_data['Date'].iloc[time_steps::50])
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()